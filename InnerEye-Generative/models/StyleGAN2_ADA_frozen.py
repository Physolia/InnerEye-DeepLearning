#  ------------------------------------------------------------------------------------------
#  Copyright (c) Microsoft Corporation. All rights reserved.
#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.
#  ------------------------------------------------------------------------------------------

# Pytorch Lightning infrastructure, bits and pieces of original StyleGAN2-ADA code 

import sys
import torch
import pickle
import numpy as np
from argparse import ArgumentParser
import torch
from pytorch_lightning import LightningModule, seed_everything, Trainer
from torch.nn import functional as F
from models.gan_loss import MinimaxLoss, SoftPlusLoss, WassersteinLoss
from prdc import compute_prdc
# Users should import calculate_frechet_distance from:
# https://github.com/mseitzer/pytorch-fid/blob/master/src/pytorch_fid/fid_score.py
# from metrics.FID import calculate_frechet_distance
import torchvision
import matplotlib.pyplot as plt



class TransferLearningStyleGAN2ADA(LightningModule):

    def __init__(
        self,
        embedder: torch.nn.Module,
        # fixed_blocks = ('b256' ,'b128', 'b64'),
        # fixed_blocks = ('b128', 'b64', 'b32'),
        fixed_blocks_exeptions = None, 
        # probalby affine.weight <- the style weights that can still be trained, 
        # but implies many parameters as they are generated by MLP from mapping
        style_mixing_prob = .9,
        **kwargs
    ):

        super().__init__()

        # makes self.hparams under the hood and saves to ckpt
        self.save_hyperparameters(ignore='embedder')

        self.fixed_blocks = self.hparams.fixed_blocks
        self.fixed_blocks_exeptions = fixed_blocks_exeptions
        self.style_mixing_prob = style_mixing_prob
        # networks
        self.embedder = embedder
        for param in self.embedder.parameters():
            param.requires_grad = False
        self.generator, self.discriminator = self.restore_model()
        if not self.hparams.transfer_learning:
            self.reset_params(self.generator)
            self.reset_params(self.discriminator)
            self.fixed_blocks = []
        # criterion
        self._get_criterion()
        # moving average a for path length reg
        self.pl_mean = torch.zeros([], device=self.device)
        import math
        self.pl_weight = float(math.log(2) / (128**2 * (math.log(128 / 2))))
        self.pl_batch_shrink = 2  # TODO why do we shrink the size of the batch for this?
        self.pl_decay = .99
        # logging
        self.z = torch.randn(16, self.generator.z_dim, device=self.device, requires_grad=True)
        #
        self.hooks: List[Any] = []

    def _get_criterion(self):
        if self.hparams.loss == 'CE':
            self.criterion = MinimaxLoss
        elif self.hparams.loss == 'SoftPlus':
            self.criterion = SoftPlusLoss
        elif self.hparams.loss == 'Wasserstein':
            self.criterion = WassersteinLoss
        else:
            raise NotImplementedError

    def restore_model(self):
        
        cached_file = self.hparams.model_weights_path
        with open(cached_file, 'rb') as f:
            models = pickle.load(f)

        gen = models['G_ema']  # taking averages over x steps of GAN weights
        # Skip the last layer
        gen.synthesis.block_resolutions = gen.synthesis.block_resolutions[:-1]
        disc = models['D']       
        return gen, disc

    def reset_params(self, model):
        for name, param in model.named_parameters():
            _type = name.rsplit('.')[-1]
            if _type == 'weight' or _type == 'const':
                torch.nn.init.normal_(param)
            elif _type == 'bias' or _type == 'noise_strength':
                torch.nn.init.zeros_(param)
            else:
                raise NotImplementedError

    def discriminator_forward(self, img, **block_kwargs):
        # augment images
        img = self.augmentations(img)

        x = None
        for res, bres_new in zip(self.discriminator.block_resolutions[:-1], [128, 64, 32, 16, 8]):
            # print(res, flush=True)
            block = getattr(self.discriminator, f'b{res}')
            # print(block.in_channels, block.resolution, x.shape if x is not None else None)
            block.resolution = bres_new
            x, img = block(x, img)
            # print(x.shape, flush=True)

        cmap = None
        x = self.discriminator.b4(x, img, cmap)
        # print(x.shape, flush=True)
        return x

    def generator_forward(self, x, z=None):
        # sample and map noise
        if z is None:
            z = torch.randn(x.shape[0], self.generator.z_dim, device=self.device, requires_grad=True)
        c = None  # no label conditioning
        # code borrowed from official StyleGAN2-ADA -- start
        ws = self.generator.mapping(z, c)
        if self.style_mixing_prob > 0:
            cutoff = torch.empty([], dtype=torch.int64, device=ws.device).random_(1, ws.shape[1])
            cutoff = torch.where(torch.rand([], device=ws.device) < self.style_mixing_prob, cutoff, torch.full_like(cutoff, ws.shape[1]))
            ws[:, cutoff:] = self.generator.mapping(torch.randn_like(z), c, skip_w_avg_update=True)[:, cutoff:]
        # code borrowed from official StyleGAN2-ADA -- end

        # generate images
        generated_imgs = self.generator.synthesis(ws)
        if self.hparams.mean_last:
            generated_imgs = torch.mean(generated_imgs, 1, keepdim=True).tile(1, 3, 1, 1) 
        return generated_imgs, ws

    def augmentations(self, img):
        # TODO
        return img

    def forward(self, z):
        """
        Note: only to be used for testing
        """
        if z in None:
            z = torch.randn(16, self.generator.z_dim, device=self.device, requires_grad=True) 
        # map noise
        w = self.generator.mapping(z, None)
        img = self.generator.synthesis(w)
        if self.hparams.mean_last:
            img = torch.mean(img, 1, keepdim=True).tile(1, 3, 1, 1) 
        return img

    def generator_loss(self, x):
        # generate images
        generated_imgs, _ = self.generator_forward(x)
        # discriminate
        D_output = self.discriminator_forward(generated_imgs)
        # calculate loss
        g_loss = self.criterion.generator_loss(D_output) #  F.binary_cross_entropy(D_output, y)
        self.log("loss/gen", float(g_loss.detach().cpu()), on_epoch=True)
        return g_loss

    def discriminator_loss(self, x):
        # train discriminator on real
        # calculate real score
        D_output_real = self.discriminator_forward(x)

        # train discriminator on fake
        x_fake, _ = self.generator_forward(x)
        # calculate fake score
        D_output_fake = self.discriminator_forward(x_fake)
        if self.hparams.debug:
            real_val = torch.sigmoid(D_output_real).mean()
            fake_val = torch.sigmoid(D_output_fake).mean()
            if real_val < 0 or real_val > 1:
                vals = torch.sigmoid(D_output_real).detach().cpu().numpy()
                print('real_val', real_val, plt.hist(vals), vals.min(), vals.max(), flush=True) 
            if fake_val < 0 or fake_val > 1:
                vals = torch.sigmoid(D_output_fake).detach().cpu().numpy()
                print('real_val', fake_val, plt.hist(vals), vals.min(), vals.max(), flush=True) 
            self.log('pred/real_min', torch.sigmoid(D_output_real).min(), on_epoch=True)
            self.log('pred/real_max', torch.sigmoid(D_output_real).max(), on_epoch=True)
            self.log('pred/fake_min', torch.sigmoid(D_output_fake).min(), on_epoch=True)
            self.log('pred/fake_max', torch.sigmoid(D_output_fake).max(), on_epoch=True) 

        self.log('pred/real', torch.sigmoid(D_output_real).mean(), on_epoch=True)
        self.log('pred/fake', torch.sigmoid(D_output_fake).mean(), on_epoch=True)
        # gradient backprop & optimize ONLY D's parameters
        D_loss = self.criterion.discriminator_loss(D_output_real, D_output_fake)
        self.log("loss/disc", D_loss, on_epoch=True)
        return D_loss

    def path_length_reg(self, x):
        # with torch.autograd.profiler.record_function('Gpl_forward'):
        batch_size = x.shape[0] // self.pl_batch_shrink
        gen_img, gen_ws = self.generator_forward(x[: batch_size])
        pl_noise = torch.randn_like(gen_img, device=self.device, requires_grad=True) / np.sqrt(gen_img.shape[2] * gen_img.shape[3])
        # todo: check this does not record gradients you don't want it to calc
        # with torch.no_grad():
        pl_grads = torch.autograd.grad(outputs=[(gen_img * pl_noise).sum()], inputs=[gen_ws], create_graph=True, only_inputs=True)[0]
        #
        pl_lengths = pl_grads.square().sum(2).mean(1).sqrt()
        self.pl_mean = self.pl_mean.to(x.device)
        pl_mean = self.pl_mean.lerp(pl_lengths.mean(), self.pl_decay)
        self.pl_mean.copy_(pl_mean.detach())
        pl_penalty = (pl_lengths - pl_mean).square()
        loss_Gpl = pl_penalty * self.pl_weight
        # self.log('loss/pl_penalty', pl_penalty.mean(), on_epoch=True)
        self.log('loss/G/reg', loss_Gpl.mean(), on_epoch=True)
        # TODO: understand why we need to multiply by gen_img * 0 ? 
        return (gen_img[:, 0, 0, 0] * 0 + loss_Gpl).mean()

    def discriminator_R1_reg(self, x):
        real_img_tmp = x.detach().requires_grad_(True)
        # train discriminator on real
        # calculate real score
        D_output_real = self.discriminator_forward(real_img_tmp)

        # train discriminator on fake
        x_fake, _ = self.generator_forward(x)
        # calculate fake score
        D_output_fake = self.discriminator_forward(x_fake)
        self.log('pred/real', torch.sigmoid(D_output_real).mean(), on_epoch=True)
        self.log('pred/fake', torch.sigmoid(D_output_fake).mean(), on_epoch=True)
        # gradient backprop & optimize ONLY D's parameters
        D_loss = self.criterion.discriminator_loss(torch.sigmoid(D_output_real), torch.sigmoid(D_output_fake))
        self.log("loss/disc", D_loss, on_epoch=True)
        # regularisation step
        # with torch.no_grad():
        r1_grads = torch.autograd.grad(outputs=[D_output_real.sum()], inputs=[real_img_tmp], create_graph=True, only_inputs=True)[0]
        r1_penalty = r1_grads.square().sum([1, 2, 3])
        reg_loss = (r1_penalty * (self.hparams.r1_gamma / 2)).mean()
        self.log('loss/D/reg', reg_loss, on_epoch=True)
        return D_loss + reg_loss

    def discriminator_gradient_penalty_reg(self, x):
        x_real = x.detach()  # .requires_grad_(True)
        x_fake, _ = self.generator_forward(x)
        x_fake = x_fake.detach()
        alpha = torch.rand([x_real.shape[0]] + [1] * len(x_real.shape[1:]),
                           device=self.device)
        x = alpha * x_real + (1. - alpha) * x_fake
        x.requires_grad = True
        probe_logit, _ = self.discriminator_forward(x)
        gradients = torch.autograd.grad(outputs=F.sigmoid(probe_logit),
                                  inputs=x,
                                  grad_outputs=torch.ones_like(probe_logit))[0]
        grad_norm = gradients.view(gradients.shape[0], -1).norm(2, dim=1)
        penalty = ((grad_norm - 1.) ** 2).mean()
        return self.hparams.r1_gamma * penalty

    def training_step(self, batch, batch_idx, optimizer_idx):
        # print(batch.shape, flush=True)
        x = batch

        # train discriminator
        result = None
        if optimizer_idx == 0:
            self.reg_iter_counters['D'] += 1
            if self.reg_iter_counters['D'] % self.reg_iter_totals['D'] == 0:
                if self.hparams.regularisation == 'R1':
                    result = self.discriminator_R1_reg(x)
                elif self.hparams.regularisation == 'gradient_penalty':
                    result = self.discriminator_gradient_penalty_reg(x)
            else:
                result = self.discriminator_loss(x)

        # train generator
        if optimizer_idx == 1:
            self.reg_iter_counters['G'] += 1
            if self.reg_iter_counters['G'] % self.reg_iter_totals['G'] == 0:
                result = self.path_length_reg(x)
            else:
                result = self.generator_loss(x)

        return result

    def validation_step(self, batch, batch_idx, dataloader_idx):
        if dataloader_idx == 0:
            # calculate metrics on holdout dataset 
            # real
            real_pred = self.discriminator_forward(batch)
            # fake
            batch_fake, _ = self.generator_forward(batch)
            fake_pred = self.discriminator_forward(batch_fake)

            real_loss, fake_loss = self.criterion.discriminator_loss(real_pred, fake_pred, breakdown=True)     
            if self.hparams.debug:
                real_val = torch.sigmoid(real_pred).mean()
                fake_val = torch.sigmoid(fake_pred).mean()
                if real_val < 0 or real_val > 1:
                    vals = torch.sigmoid(real_pred).detach().cpu().numpy()
                    print('real_val', real_val, plt.hist(vals), vals.min(), vals.max(), flush=True) 
                if fake_val < 0 or fake_val > 1:
                    vals = torch.sigmoid(fake_pred).detach().cpu().numpy()
                    print('real_val', fake_val, plt.hist(vals), vals.min(), vals.max(), flush=True) 

            self.log('pred/real_val',torch.sigmoid(real_pred).mean(), on_epoch=True)
            self.log('pred/fake_val', torch.sigmoid(fake_pred).mean(), on_epoch=True)
            self.log("loss/disc_real", real_loss, on_epoch=True)
            self.log("loss/disc_fake", fake_loss, on_epoch=True)
            return batch_fake

        elif dataloader_idx == 1:
            self.embedder.to(self.device)
            with torch.no_grad():
                batch_emb = self.embedder(batch)

            batch_fake, _ = self.generator_forward(batch)
            with torch.no_grad():
                fake_emb = self.embedder(batch_fake)
            out = torch.stack((batch_emb, fake_emb), -1)
            return out

    def validation_epoch_end(self, outputs) -> None:
        val_out, train_out = outputs
        # use val out to save some images and train out to calculate the metrics 

        train_out = torch.cat(train_out, 0)
        train_out = np.array(train_out.detach().cpu())
        # precision, recall, density, coverage
        metrics = compute_prdc(train_out[:, :, 0], train_out[:, :, 1], self.hparams.k_neigh)
        # FID
        mus = [np.mean(train_out[:, :, i], axis=0) for i in range(2)]
        sigmas = [np.cov(train_out[:, :, i], rowvar=False) for i in range(2)]
        metrics['FID'] = calculate_frechet_distance(mus[0], sigmas[0], mus[1], sigmas[1])
        # log
        self.log_dict({'metrics/' + key: metrics[key] for key in metrics}, on_epoch=True)

        # log random images
        val_out = torch.cat(val_out, 0)
        grid = torchvision.utils.make_grid(val_out[-16:])
        grid = torch.clip((grid + 1) * .5, 0, 1)
        self.logger.log_image('gen_imgs_random', grid, step=self.global_step)
        # log set images
        if self.z.device != self.device:
            self.z = self.z.to(self.device)
        gen_imgs, _ = self.generator_forward(None, self.z)
        grid = torchvision.utils.make_grid(gen_imgs)
        grid = torch.clip((grid + 1) * .5, 0, 1)
        self.logger.log_image('gen_imgs_set', grid, step=self.global_step)   
        if self.hparams.save_model_manually:
            torch.save(self.state_dict(), './outputs/model.pt')

    def select_params(self, model):
        params = []
        for name, param in model.named_parameters():
            cond = np.sum([fixed_block in name for fixed_block in self.fixed_blocks]) == 0
            if self.fixed_blocks_exeptions is not None:
                cond = cond and np.sum([block_exep not in name for block_exep in self.fixed_blocks_exeptions]) == 0
                param.requires_grad = False
            if cond:
                param.requires_grad = True
                params.append(param)
        return params

    def configure_optimizers(self):
        lr_base = self.hparams.learning_rate
        betas = [self.hparams.beta1, 0.999]
        # hyperparam: optimise G / D at different rates
        optimiser_step_ratio = self.hparams.optimiser_step_ratio
        # transfer learning: only optimise the higher level weights
        generator_params = self.select_params(self.generator)
        discriminator_params = self.select_params(self.discriminator)
        # Regularisation: only regularise each reg_interval iterations
        ## Gen
        mb_ratio = self.hparams.reg_interval_G / (self.hparams.reg_interval_G + 1)
        lr = lr_base * mb_ratio
        betas = [beta ** mb_ratio for beta in betas]
        opt_g = torch.optim.Adam(generator_params, lr=lr, betas=betas)
        ## Dis
        mb_ratio = self.hparams.reg_interval_D / (self.hparams.reg_interval_D + 1)
        lr = lr_base * mb_ratio
        betas = [beta ** mb_ratio for beta in betas]        
        opt_d = torch.optim.Adam(discriminator_params, lr=lr, betas=betas)
        ## in theory we could add two optimiser indices and ask lightning to do the job for us
        ## in practice, we cannot nest orders of optimisation, hence we will do it manually by 
        ## adding counters
        self.reg_iter_counters = {'D': 0, 'G': 0}
        self.reg_iter_totals = {'D': self.hparams.reg_interval_D, 'G': self.hparams.reg_interval_G}
        if optimiser_step_ratio >= 1:
            return ({'optimizer': opt_d, 'frequency': int(optimiser_step_ratio)},
                    {'optimizer': opt_g, 'frequency': 1})
        else:
            return ({'optimizer': opt_d, 'frequency': 1},
                    {'optimizer': opt_g, 'frequency': int(1/optimiser_step_ratio)}) 

    @staticmethod
    def add_model_specific_args(parent_parser):
        parser = ArgumentParser(parents=[parent_parser], add_help=False)
        parser.add_argument("--beta1", default=0.5, type=float, help="Adam's beta1 value. This hyperparm. controls both the generator and the discriminator optimisers")
        parser.add_argument("--optimiser_step_ratio", "-osr", default=1, type=float, help="""Ratio of discriminator seps to generator steps. 
        In the literature, the discriminator is trained for more iterations than the generator. 
        Here, a positive number will train more often the discriminator, whereas a negative number will train more often the generator""")
        parser.add_argument('--learning_rate', type=float, default=0.0002, help="adam: learning rate")
        parser.add_argument("--k_neigh", default=5, type=int, help="""Number of neighbours to consider for the precision / recall metrics. 
        These metrics fit a probability map over the dataset embedding through k-nearest-neighbours.""")
        parser.add_argument('--model_weights_path', '-w', type=str, help="""Slight abuse of notation: instead of weights, this is the path for a saved model. 
        The script defaults to an NVIDIA model pretrained on a faces dataset. 
        To be precise, the model loaded is an intermediate model trained on images of downsized resolution 256x256.
        In NVIDIA script it is then meant to be further trained with full resolution images, adding a resolution block to both the generator and discriminator""")
        parser.add_argument("--loss", default='Wasserstein', type=str, choices=('CE', 'SoftPlus', 'Wasserstein'), 
            help="""Type of loss used. Note that the Wasserstein loss here is applied without softmax. 
        This was an error which seemed to have a low impact on results.
        However, it means the discriminator's probabilities do not hover around .5 once it is close to equilibrium""")
        parser.add_argument("--reg_interval_G", default=8, type=int, help="Regularisation interval for the generator. Kept paper's defaults")
        parser.add_argument("--reg_interval_D", default=16, type=int, help="Regularisation interval for the discriminator. Kept paper's defaults")
        parser.add_argument('--r1_gamma', default=10, type=float, help="Strength of discriminator L2 reg. Kept paper's defaults")
        parser.add_argument('--regularisation', '-reg', default='R1', choices=('R1', 'gradient_penalty'), type=str,
            help="Type of regularisation between R1 regularisation used by StylGAN2 and gradient penalty used by DCGAN")
        parser.add_argument('--fixed_blocks', nargs='+', type=str, default=['b256', 'b128'], 
            help="""Blocks of the generator and discriminator to freeze for training. The blocks go from b4 to b256 with steps defined by powers of 2.
        Note that the generator's last block b256 has been removed (line 82) in order to generate scans of dimension 128 x 128.
        Similarly, the block v4 has been removed from the discriminator.
        Note that the default is what was used on the tuned model""")
        parser.add_argument('--transfer_learning', '-tl', action='store_false', default=True, 
            help="Calling this flag turns off transfer learning and re-initialises all weights randomly, so that you can train the model from scratch")
        parser.add_argument('--save_model_manually', '-s', action='store_true', default=False,
            help="Saves the model at the end of each validation pass")
        parser.add_argument('--mean_last', action='store_true', default=False, 
            help="""The generator produces an output with three channels 3x128x128. 
        This flag means that the three chanels will be averaged before being passed to the discriminator""")
        # todo: understand and add pl_batch_shrink=2, pl_decay=0.01, pl_weight=2
        return parser

    def forward_hook_fn(self, module: torch.nn.Module, input: torch.Tensor, output: torch.Tensor) -> None:

        """
        Forward hook to save the activations of a given layer (per device to allow GradCam to be computed
        with DataParallel models when training on multiple GPUs.
        """
        device = str(output[0].get_device())
        if device not in self.activations:
            self.activations[device] = []
        if isinstance(output, tuple):
            self.activations[device].append([output[index].data.clone() for index in range(len(output))])
        else:
            self.activations[device].append(output.data.clone())


class TransferLearningStyleGAN2ADA3D(TransferLearningStyleGAN2ADA):
    def __init__(self,
        embedder: torch.nn.Module,
        fixed_blocks_exeptions = None, 
        style_mixing_prob = .9,
        **kwargs):
        super().__init__(embedder, fixed_blocks_exeptions, style_mixing_prob, **kwargs)
        if self.hparams.mean_last:
            self.hparams.mean_last = False
            raise Warning("Taking the mean over different channels is against the point of the 3D volume. Setting 'mean_last' parameter to false")

    def validation_epoch_end(self, outputs) -> None:
        val_out, train_out = outputs
        # use val out to save some images and train out to calculate the metrics 

        train_out = torch.cat(train_out, 0)
        train_out = np.array(train_out.detach().cpu())
        # precision, recall, density, coverage
        metrics = compute_prdc(train_out[:, :, 0], train_out[:, :, 1], self.hparams.k_neigh)
        # FID
        mus = [np.mean(train_out[:, :, i], axis=0) for i in range(2)]
        sigmas = [np.cov(train_out[:, :, i], rowvar=False) for i in range(2)]
        metrics['FID'] = calculate_frechet_distance(mus[0], sigmas[0], mus[1], sigmas[1])
        # log
        self.log_dict({'metrics/' + key: metrics[key] for key in metrics}, on_epoch=True)

        # log random images
        val_out = torch.cat(val_out, 0)
        val_out = val_out.reshape(val_out.shape[0] * val_out.shape[1], val_out.shape[2], val_out.shape[3])
        grid = torchvision.utils.make_grid(val_out[-16:])
        grid = torch.clip((grid + 1) * .5, 0, 1)
        self.logger.log_image('gen_imgs_random', grid, step=self.global_step)
        # log set images
        if self.z.device != self.device:
            self.z = self.z.to(self.device)
        gen_imgs, _ = self.generator_forward(None, self.z)
        gen_imgs = gen_imgs.reshape(gen_imgs.shape[0] * gen_imgs.shape[1], gen_imgs.shape[2], gen_imgs.shape[3])
        grid = torchvision.utils.make_grid(gen_imgs)
        grid = torch.clip((grid + 1) * .5, 0, 1)
        self.logger.log_image('gen_imgs_set', grid, step=self.global_step)   
        if self.hparams.save_model_manually:
            torch.save(self.state_dict(), './outputs/model.pt')

    def training_step(self, batch, batch_idx, optimizer_idx):
        return super().training_step(batch, batch_idx, optimizer_idx)